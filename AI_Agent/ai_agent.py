# -*- coding: utf-8 -*-
"""Ai_Agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-Hm5BgBs-Bj1b_dSKqFruXQDGtnYYXi-

# *実行する前にGPUを選んでください

# ライブラリをインストール
"""

!pip install -U accelerate peft bitsandbytes transformers trl torch torchvision torchaudio langchain-community langchain-huggingface langchain duckduckgo-search

"""# ライブラリをロード"""

from langchain_huggingface import HuggingFacePipeline
import transformers
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    pipeline
)
import torch
from langchain_community.tools import DuckDuckGoSearchResults
from langchain_community.utilities import DuckDuckGoSearchAPIWrapper
from langchain_core.prompts import PromptTemplate
from langchain_core.tools import Tool
from langchain.chains import LLMChain
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

"""# 生成AIをHuggingFaceからロード"""

hf_auth = HF_TOKENS  # ここにHuggingFaceのトークンを入れる
model_id = 'elyza/ELYZA-japanese-Llama-2-7b-instruct'

# 量子化
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,  # 4bitのQuantizationの有効化
    bnb_4bit_quant_type="nf4",  # 4bitのQuantizationのタイプ (fp4 or nf4)
    bnb_4bit_compute_dtype=torch.float16,  # 4bitのQuantizationのdtype (float16 or bfloat16)
    bnb_4bit_use_double_quant=False,  # 4bitのDouble-Quantizationの有効化
)

model_config = transformers.AutoConfig.from_pretrained(
    model_id,
    token=hf_auth
)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    trust_remote_code=True,
    config=model_config,
    quantization_config=quantization_config,
    device_map='auto',
    token=hf_auth,
    weights_only=True,
)

# Tokenizerの準備
tokenizer = AutoTokenizer.from_pretrained(
    model_id,  # モデル名
    use_fast=False,  # Fastトークナイザーの有効化
    add_eos_token=True,  # データへのEOSの追加を指示
    trust_remote_code=True,
    token=hf_auth,
)

pipe = pipeline("text-generation",
                model=model,
                tokenizer=tokenizer
            )
hf_llm = HuggingFacePipeline(pipeline=pipe)

"""# 検索エンジンを使用してAIエージェントを構築"""

def search_web(query: str) -> str:
    wrapper = DuckDuckGoSearchAPIWrapper(region="jp-jp", time="d", max_results=2)
    engine = DuckDuckGoSearchResults(api_wrapper=wrapper, backend="news")
    return engine.invoke(f"{query}")

prompt_template = ChatPromptTemplate([
    ("system", "あなたはAIアシスタントです。必要に応じて、検索結果を使用します。"),
    ("user", "{question}")
])

chain = ({"question": lambda x: x["question"], "検索結果": lambda x: search_web(x["question"])} # <----search_webを使用
         | prompt_template
         | hf_llm
         | StrOutputParser()
         )
question = "トヨタとは？"
print(chain.invoke({"question": question}))

"""# # 検索エンジンを使用せずにAIエージェントを構築"""

prompt_template = ChatPromptTemplate([
    ("system", "あなたはAIアシスタントです。"),
    ("user", "{question}")
])
chain = (prompt_template
         | hf_llm
         | StrOutputParser()
         )
question = "トヨタとは？"
print(chain.invoke({"question": question}))