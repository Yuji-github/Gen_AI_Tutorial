# -*- coding: utf-8 -*-
"""LLAMA_Fine_Tune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YCG8NlrF1iILMp362RoY99esnKVKaqFu

# *実行する前にGPUを選んでください

# ライブラリをインストール
"""

!pip install -U accelerate peft bitsandbytes transformers trl torch torchvision torchaudio

"""# ライブラリをロード"""

import torch
from torch import cuda
from datasets import load_dataset
import transformers
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
)
import peft
from peft import LoraConfig
from trl import SFTTrainer

"""# プロンプトテンプレートの準備"""

def generate_prompt(data_point):
    if data_point["input"]:
        result = f"[INST] {data_point['instruction']}\n\n{data_point['input']} [/INST] {data_point['output']}"
    else:
        result = f"[INST] {data_point['instruction']} [/INST] {data_point['output']}"
    return result


# text列の追加
def add_text(example):
    example["text"] = generate_prompt(example)
    for key in ["category", "instruction", "input", "output"]:
        del example[key]
    return example

dataset = load_dataset("saldra/sakura_japanese_dataset")
dataset["train"][0]["instruction"]

"""# データセットの分割"""

dataset = dataset.map(add_text)
train_val = dataset["train"].train_test_split(
    test_size=0.2, shuffle=True, seed=42
)
train_data = train_val["train"]
val_data = train_val["test"]
train_data[0]["text"]

"""# 生成AIをHuggingFaceからロード"""

hf_auth = 'HF_AUTH_TOKEN'  # ここにHuggingFaceのトークンを入れる
model_id = 'elyza/ELYZA-japanese-Llama-2-7b-instruct'

# 量子化
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,  # 4bitのQuantizationの有効化
    bnb_4bit_quant_type="nf4",  # 4bitのQuantizationのタイプ (fp4 or nf4)
    bnb_4bit_compute_dtype=torch.float16,  # 4bitのQuantizationのdtype (float16 or bfloat16)
    bnb_4bit_use_double_quant=False,  # 4bitのDouble-Quantizationの有効化
)

# モデルのロード
model_config = transformers.AutoConfig.from_pretrained(
    model_id,
    use_auth_token=hf_auth
)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    trust_remote_code=True,
    config=model_config,
    quantization_config=quantization_config,
    device_map='auto',
    use_auth_token=hf_auth,
    weights_only=True
)
model.config.use_cache = False  # キャッシュ (学習時はFalse)
model.config.pretraining_tp = 1  # 事前学習で使用したTensorランク

"""## Tokanizerを作成"""

# Tokenizerの準備
tokenizer = AutoTokenizer.from_pretrained(
    model_id,  # モデル名
    use_fast=False,  # Fastトークナイザーの有効化
    add_eos_token=True,  # データへのEOSの追加を指示
    trust_remote_code=True,
    use_auth_token=hf_auth,
)
tokenizer.pad_token = tokenizer.unk_token
tokenizer.padding_side = "right" # fp16でのオーバーフロー問題対策

"""## LoRAの作成"""

peft_config = LoraConfig(
    r=4,  # LoRAアテンションの次元
    lora_alpha=16,  # LoRAスケーリングのAlphaパラメータ
    lora_dropout=0.05,  # LoRA レイヤーのドロップアウト確率
    bias="none",  # LoRAのバイアス種別 ("none","all", "lora_only")
    task_type="CAUSAL_LM",  # タスク種別
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj", "embed_tokens", "lm_head"],
)
model = peft.get_peft_model(model, peft_config)

"""# 生成AIをファインチューンする

## トレーナーの作成
"""

eval_steps = 50
save_steps = 400
logging_steps = 400
max_steps = 400 # dollyだと4881
output_dir = './'

trainer = SFTTrainer(
    model=model,
    train_dataset=train_data,
    eval_dataset=val_data,
    args=transformers.TrainingArguments(
        num_train_epochs=3,
        fp16=True,  # fp16学習の有効化
        bf16=False,  # bf16学習の有効化
        optim="paged_adamw_32bit",  # オプティマイザ
        learning_rate=3e-4,   # 初学習率
        lr_scheduler_type="cosine",  # 学習率スケジュール
        max_grad_norm=0.3,  # 最大法線勾配 (勾配クリッピング)
        warmup_ratio=0.03,  # 線形ウォームアップのステップ比率 (0から学習率まで)
        weight_decay=0.001,  # bias/LayerNormウェイトを除く全レイヤーに適用するウェイト減衰
        logging_steps=logging_steps, # nステップ毎にログを記録する
        eval_strategy="steps",
        save_strategy="steps",
        max_steps=max_steps, # 学習ステップ数
        eval_steps=eval_steps, # nステップ毎にEvalをする
        save_steps=save_steps, # nステップ毎にチェックポイントを保存
        output_dir=output_dir, # 出力ディレクトリ
        report_to="none",
        save_total_limit=3,
        push_to_hub=False,
        auto_find_batch_size=True
    ),
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),
)

"""## トレーニング"""

trainer.train()

# LoRAモデルの保存
trainer.model.save_pretrained('myLoRA_model')

"""# ファインチューンした生成AIを試す"""

val_data[0]["text"]

"""観葉植物から植物に変更して試す"""

prompt = '[INST] ジョーは植物を買った。 彼はそれが家のどの部分に似合うと思ったか？ 家族部屋、熱帯林、花畑、映画館、ヘアサロン [/INST]'

input_ids = tokenizer(prompt, add_special_tokens=False, return_tensors='pt')
output_ids = model.generate(
    **input_ids.to(model.device),
    max_new_tokens=100,
    do_sample=True,
    temperature=0.3,
)
output = tokenizer.decode(output_ids.tolist()[0])
print(output)