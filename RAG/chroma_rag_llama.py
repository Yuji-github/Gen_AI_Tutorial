# -*- coding: utf-8 -*-
"""Chroma_RAG_LLAMA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13ahWlitsbYyiInyxQ9N-jRYd5KV9OHDG

# *実行する前にGPUを選んでください

# ChromaのPythonライブラリをインストール
"""

!pip install chromadb

!pip install langchain_community

!pip install bitsandbytes  # モデルを小さくする為に必要

"""# ライブラリをロード"""

import bs4
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma  # Chromaはlangchain_communityがChromaDBを管理

"""# Web スクレイピング"""

url = 'https://www.marktechpost.com/2025/06/11/nvidia-researchers-introduce-dynamic-memory-sparsification-dms-for-8x-kv-cache-compression-in-transformer-llms/'

# スクレイピング
loader = WebBaseLoader(
    web_paths=(f"{url}",),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            class_=("td-post-content tagdiv-type", "td-post-header", "td-post-title")
        ) # このclass_はウェブサイトによって異なる
    ),
)
docs = loader.load()
docs

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(docs)  # スクレイピングしたテキストを分割
splits

"""# 生成AIをHuggingFaceからロード"""

import transformers
from torch import cuda
from transformers import BitsAndBytesConfig

model_id = 'NousResearch/Llama-2-7b-chat-hf'
device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'
print(f"Using {device}")

quantization_config = BitsAndBytesConfig(load_in_8bit=True)
hf_auth = 'HF_AUTH_TOKEN'  # ここにHuggingFaceのトークンを入れる
model_config = transformers.AutoConfig.from_pretrained(
    model_id,
    use_auth_token=hf_auth
)

model = transformers.AutoModelForCausalLM.from_pretrained(
    model_id,
    trust_remote_code=True,
    config=model_config,
    quantization_config=quantization_config,
    device_map='auto',
    use_auth_token=hf_auth
)
model.eval()

"""## Tokanizerを作成"""

tokenizer = transformers.AutoTokenizer.from_pretrained(
    model_id,
    use_auth_token=hf_auth
)

"""## Pipelineを作成"""

generate_text = transformers.pipeline(
    model=model, tokenizer=tokenizer,
    return_full_text=True,  # 全文が入る
    task='text-generation',
    # パラメーター
    temperature=0.1,  # クリエイティブ量、高ければハルシネーションが起きる　（０－１）
    max_new_tokens=512,  # 生成できる量
    repetition_penalty=1.1  # 同じものを作成する際のペナルティー
)

"""# Chromaを使いVector Databaseの作成

Embedding方法はVector　Databaseに保存目的なので何でもOK（生成AIのAPIを使う必要はない）
"""

from langchain_community.embeddings import HuggingFaceEmbeddings

model_name = "sentence-transformers/all-mpnet-base-v2"  # sentence-transformers系ならなんでも良い
model_kwargs = {'device': 'cuda'}
encode_kwargs = {'normalize_embeddings': False}
hf = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)
vectorstore = Chroma.from_documents(documents=splits, embedding=hf)
vectorstore.get()

"""# RAGを使い生成AIに聞く"""

from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA

llm = HuggingFacePipeline(pipeline=generate_text)
rag_pipeline = RetrievalQA.from_chain_type(
    llm=llm, chain_type='stuff',
    retriever=vectorstore.as_retriever()
)

rag_pipeline('What is Dynamic Memory Sparsification?')