# -*- coding: utf-8 -*-
"""LLAMA_Knowledge_Distillation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yM99kDIscbFpbZVGH4pLh_DUkRCY-mvM

# *実行する前にGPUを選んでください

# ライブラリをインストール
"""

!pip install -U bitsandbytes transformers datasets

"""# ライブラリをロード"""

from typing import List, Tuple
import torch
from torch import cuda
from datasets import load_dataset
import transformers
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig
)
import math
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torch.optim import AdamW
from transformers import get_cosine_schedule_with_warmup

"""# データセットの準備"""

#　アウトプットは外すー＞生徒モデルが教師モデルのオリジナルの答えをコピーする為
def generate_prompt(data_point):
    if data_point["input"]:
        result = f"[INST] {data_point['instruction']}\n\n{data_point['input']} [/INST]"
    else:
        result = f"[INST] {data_point['instruction']} [/INST]"
    return result


# text列の追加
def add_text(example):
    example["text"] = generate_prompt(example)
    for key in ["category", "instruction", "input", "output"]:
        del example[key]
    return example

#　DataLoaderが呼ばれた時にトークン化させる
def llama_collate_fn(batch: List[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:
  x = [item['text'] for item in train_data]
  tokenized_batch = tokenizer(x, max_length=64, return_tensors='pt', padding="max_length", truncation=True)
  return tokenized_batch

"""## データセットの分割"""

dataset = load_dataset("saldra/sakura_japanese_dataset")
dataset = dataset.map(add_text)
train_val = dataset["train"].train_test_split(
    test_size=0.2, shuffle=True, seed=42
)
train_data = train_val["train"]
val_data = train_val["test"]
train_dataloader = DataLoader(
        train_data,
        batch_size=2,
        pin_memory=True,
        collate_fn=llama_collate_fn
    )

"""# 生成AIをHuggingFaceからロード"""

hf_auth = 'HF_AUTH_TOKEN'  # ここにHuggingFaceのトークンを入れる
teacher_model_id = 'elyza/ELYZA-japanese-Llama-2-7b-instruct'
student_model_id = "TinyLlama/TinyLlama_v1.1"

"""## Tokanizerを作成"""

tokenizer = AutoTokenizer.from_pretrained(
    teacher_model_id,  # モデル名
    use_fast=False,  # Fastトークナイザーの有効化
    add_eos_token=True,  # データへのEOSの追加を指示
    trust_remote_code=True,
    token=hf_auth,
)
tokenizer.pad_token = tokenizer.unk_token
tokenizer.padding_side = "right" # fp16でのオーバーフロー問題対策

"""## 教師モデルのロード・テスト"""

# モデルのロード
model_config = transformers.AutoConfig.from_pretrained(
    teacher_model_id,
    use_auth_token=hf_auth
)

# 量子化
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,  # 4bitのQuantizationの有効化
    bnb_4bit_quant_type="nf4",  # 4bitのQuantizationのタイプ (fp4 or nf4)
    bnb_4bit_compute_dtype=torch.float16,  # 4bitのQuantizationのdtype (float16 or bfloat16)
    bnb_4bit_use_double_quant=False,  # 4bitのDouble-Quantizationの有効化
)

teacher_model = AutoModelForCausalLM.from_pretrained(
    teacher_model_id,
    trust_remote_code=True,
    config=model_config,
    quantization_config=quantization_config,
    device_map='auto',
    token=hf_auth,
    weights_only=True
)
teacher_model.eval()

prompt = '[INST] 張師は1時間に42個の部品を加工し、李師は張師の3倍である27個の部品を加工しています。李さんは1時間に何個の部品を加工していますか？ [/INST]'
input_ids = tokenizer(prompt, add_special_tokens=False, return_tensors='pt')
output_ids = teacher_model.generate(
    **input_ids.to(teacher_model.device),
    max_new_tokens=100,
    do_sample=True,
    temperature=0.3,
)
output = tokenizer.decode(output_ids.tolist()[0])
print(output)

"""## 生徒モデルのロード・テスト"""

student_model = AutoModelForCausalLM.from_pretrained(
    student_model_id,
    trust_remote_code=True,
    torch_dtype="auto",
    token=hf_auth,
    device_map='auto',
    weights_only=True)

student_model.config.use_cache = False  # キャッシュ (学習時はFalse)
student_model.config.pretraining_tp = 1  # 事前学習で使用したTensorランク

student_model.eval()

prompt = '[INST] 張師は1時間に42個の部品を加工し、李師は張師の3倍である27個の部品を加工しています。李さんは1時間に何個の部品を加工していますか？ [/INST]'
input_ids = tokenizer(prompt, add_special_tokens=False, return_tensors='pt')
output_ids2 = student_model.generate(
    **input_ids.to(student_model.device),
    max_new_tokens=100,
    do_sample=True,
    temperature=0.3,
)
output2 = tokenizer.decode(output_ids2.tolist()[0])
print(output2)

"""# 蒸留損失（Distillation Loss）作成"""

def distillation_loss(student_logits, teacher_logits, temperature:float = 2.0):
    return F.kl_div(
        F.log_softmax(student_logits / temperature, dim=-1),
        F.softmax(teacher_logits / temperature, dim=-1),
        reduction='batchmean'
    ) * (temperature ** 2)

"""# トレーニング"""

def train(device,
          student_model,
          teacher_model,
          train_dataloader,
          lr: float = 3e-4,
          warmup_steps: int = 4000,
          n_epochs:int=2,
          temperature: float = 2,
          accumulation_steps: int = 8):

    student_model = student_model.to(device)
    teacher_model = teacher_model.to(device)
    teacher_model.eval()

    optimizer = AdamW(student_model.parameters(), lr=lr)

    # トレーニングステップ
    total_steps = len(train_dataloader) * n_epochs

    # コサインスケジュラーの作成
    scheduler = get_cosine_schedule_with_warmup(
        optimizer=optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps
    )

    start_epoch, start_step = 0, 0

    for epoch in range(n_epochs):
        student_model.train()

        optimizer.zero_grad()
        for step, inputs in enumerate(train_dataloader, start=start_step):
            inputs = inputs.to(device)

            student_logits = student_model(**inputs).logits
            with torch.no_grad():
                teacher_logits = teacher_model(**inputs).logits

            distill_loss = distillation_loss(student_logits, teacher_logits, temperature)
            distill_loss.backward()

            if (step + 1) % accumulation_steps == 0:
                optimizer.step()
                optimizer.zero_grad()
                scheduler.step()

        # リセット start_stepを0
        if epoch == start_epoch:
            start_step = 0

    return student_model

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
train(
      device=device,
      student_model=student_model,
      teacher_model=teacher_model,
      train_dataloader=train_dataloader,
  )